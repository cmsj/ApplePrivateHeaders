/*
* This header is generated by classdump-dyld 1.0
* on Wednesday, April 28, 2021 at 2:52:52 PM British Summer Time
* Operating System: Version 11.3 (Build 20E232)
* Image Source: /System/Library/PrivateFrameworks/SiriSpeechSynthesis.framework/Versions/A/SiriSpeechSynthesis
* classdump-dyld is licensed under GPLv3, Copyright Â© 2013-2016 by Elias Limneos.
*/

#import <SiriSpeechSynthesis/SiriSpeechSynthesis-Structs.h>
#import <libobjc.A.dylib/FLTBFBufferAccessor.h>
#import <libobjc.A.dylib/NSCopying.h>

@class NSMutableDictionary, NSData, QSSFinalSpeechRecognitionResponse, QSSPartialSpeechRecognitionResponse, QSSUpdatedAcousticProfile, QSSEndPointLikelihood, QSSEndPointCandidate, QSSRecognitionProgress, QSSCheckForSpeechResponse, QSSRecognitionCandidate, QSSRequestStatsResponse, QSSServerEndpointFeatures, QSSClientSetupInfo, QSSAudioLimitExceeded;

@interface QSSRecognitionStreamingResponse : NSObject <FLTBFBufferAccessor, NSCopying> {

	NSMutableDictionary* _storage;
	NSData* _data;
	const RecognitionStreamingResponse* _root;

}

@property (nonatomic,readonly) long long content_type; 
@property (nonatomic,readonly) QSSFinalSpeechRecognitionResponse * contentAsQSSFinalSpeechRecognitionResponse; 
@property (nonatomic,readonly) QSSPartialSpeechRecognitionResponse * contentAsQSSPartialSpeechRecognitionResponse; 
@property (nonatomic,readonly) QSSUpdatedAcousticProfile * contentAsQSSUpdatedAcousticProfile; 
@property (nonatomic,readonly) QSSEndPointLikelihood * contentAsQSSEndPointLikelihood; 
@property (nonatomic,readonly) QSSEndPointCandidate * contentAsQSSEndPointCandidate; 
@property (nonatomic,readonly) QSSRecognitionProgress * contentAsQSSRecognitionProgress; 
@property (nonatomic,readonly) QSSCheckForSpeechResponse * contentAsQSSCheckForSpeechResponse; 
@property (nonatomic,readonly) QSSRecognitionCandidate * contentAsQSSRecognitionCandidate; 
@property (nonatomic,readonly) QSSRequestStatsResponse * contentAsQSSRequestStatsResponse; 
@property (nonatomic,readonly) QSSServerEndpointFeatures * contentAsQSSServerEndpointFeatures; 
@property (nonatomic,readonly) QSSClientSetupInfo * contentAsQSSClientSetupInfo; 
@property (nonatomic,readonly) QSSAudioLimitExceeded * contentAsQSSAudioLimitExceeded; 
-(QSSFinalSpeechRecognitionResponse *)contentAsQSSFinalSpeechRecognitionResponse;
-(QSSPartialSpeechRecognitionResponse *)contentAsQSSPartialSpeechRecognitionResponse;
-(QSSUpdatedAcousticProfile *)contentAsQSSUpdatedAcousticProfile;
-(QSSEndPointLikelihood *)contentAsQSSEndPointLikelihood;
-(QSSEndPointCandidate *)contentAsQSSEndPointCandidate;
-(QSSRecognitionProgress *)contentAsQSSRecognitionProgress;
-(QSSCheckForSpeechResponse *)contentAsQSSCheckForSpeechResponse;
-(QSSRecognitionCandidate *)contentAsQSSRecognitionCandidate;
-(QSSRequestStatsResponse *)contentAsQSSRequestStatsResponse;
-(QSSServerEndpointFeatures *)contentAsQSSServerEndpointFeatures;
-(QSSClientSetupInfo *)contentAsQSSClientSetupInfo;
-(QSSAudioLimitExceeded *)contentAsQSSAudioLimitExceeded;
-(id)copyWithZone:(NSZone*)arg1 ;
-(id)flatbuffData;
-(id)initWithFlatbuffData:(id)arg1 root:(const RecognitionStreamingResponse*)arg2 verify:(char)arg3 ;
-(Offset<siri::speech::qss_fb::RecognitionStreamingResponse>)addObjectToBuffer:(FlatBufferBuilder*)arg1 ;
-(id)initWithFlatbuffData:(id)arg1 ;
-(id)initAndVerifyWithFlatbuffData:(id)arg1 ;
-(id)initWithFlatbuffData:(id)arg1 root:(const RecognitionStreamingResponse*)arg2 ;
-(long long)content_type;
@end

