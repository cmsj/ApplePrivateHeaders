/*
* This header is generated by classdump-dyld 1.0
* on Wednesday, April 28, 2021 at 2:51:07 PM British Summer Time
* Operating System: Version 11.3 (Build 20E232)
* Image Source: /System/Library/Frameworks/MLCompute.framework/Versions/A/MLCompute
* classdump-dyld is licensed under GPLv3, Copyright Â© 2013-2016 by Elias Limneos.
*/

#import <MLCompute/MLCompute-Structs.h>
#import <MLCompute/MLCLayerOperations.h>
#import <MLCompute/MLCEngineDispatch.h>
#import <libobjc.A.dylib/NSCopying.h>
#import <MLCompute/MLComputeEngineControl.h>

@class NSArray, NSMutableArray, NSString;

@interface MLCDeviceGPU : NSObject <MLCLayerOperations, MLCEngineDispatch, NSCopying, MLComputeEngineControl> {

	char _isAppleSoCGPU;
	int _deviceType;
	os_unfair_lock_s _l;
	NSArray* _deviceList;
	NSMutableArray* _gpuHeapTemporaryResourceAllocator;
	NSMutableArray* _gpuCommandQueueList;
	NSMutableArray* _gpuCommandBufferList;
	NSMutableArray* _gpuMPSCommandBufferList;
	NSMutableArray* _gpuWriteTensorKernelList;
	NSMutableArray* _gpuReadTensorKernelList;
	NSMutableArray* _gpuLibrary;
	NSMutableArray* _gpuPipelineStatesForwardConcat2DArray;
	NSMutableArray* _gpuPipelineStatesForwardConcat2D;
	NSMutableArray* _gpuPipelineStatesConcatFloat;
	NSMutableArray* _gpuPipelineStatesConcatBool;
	NSMutableArray* _gpuPipelineStatesFill2DArray;
	NSMutableArray* _gpuPipelineStatesFill2D;
	NSMutableArray* _gpuPipelineStatesPad2DArray;
	NSMutableArray* _gpuPipelineStatesPad2D;
	NSMutableArray* _gpuPipelineStatesReduceAcrossBatch;
	NSMutableArray* _gpuPipelineStatesArithmeticUnaryForward;
	NSMutableArray* _gpuPipelineStatesArithmeticUnaryGradient;
	NSMutableArray* _gpuPipelineStatesArithmeticBinaryForward;
	NSMutableArray* _gpuPipelineStatesArithmeticBinaryGradient;
	NSMutableArray* _gpuPipelineStatesArithmeticReduceGradientAny;
	NSMutableArray* _gpuPipelineStatesArithmeticReduceGradientAll;
	NSMutableArray* _gpuPipelineStatesEmbeddingRenormalizeWeights;
	NSMutableArray* _gpuPipelineStatesEmbeddingForward;
	NSMutableArray* _gpuPipelineStatesEmbeddingGradient;
	NSMutableArray* _gpuPipelineStatesCompareRelationalOpForward;
	NSMutableArray* _gpuPipelineStatesCompareLogicalOpForward;
	NSMutableArray* _gpuPipelineStatesMHAMaskForward;
	NSMutableArray* _gpuPipelineStatesMemFillFloat;
	NSMutableArray* _gpuPipelineStatesMemCopy;
	NSMutableArray* _gpuConcurrentEncoderFence;
	double* _executionTimeInterval;
	NSArray* _gpuLocalEventList;
	NSArray* _gpuSharedEventList;
	unsigned long long* _currentEventValue;
	unsigned long long _numDevicesToUse;
	unsigned long long _numDevicesUsedWithFirstBatch;

}

@property (readonly) unsigned long long hash; 
@property (readonly) Class superclass; 
@property (copy,readonly) NSString * description; 
@property (copy,readonly) NSString * debugDescription; 
@property (assign,l,nonatomic) os_unfair_lock_s l;                                                       //@synthesize l=_l - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuHeapTemporaryResourceAllocator;                         //@synthesize gpuHeapTemporaryResourceAllocator=_gpuHeapTemporaryResourceAllocator - In the implementation block
@property (nonatomic,readonly) double* executionTimeInterval;                                            //@synthesize executionTimeInterval=_executionTimeInterval - In the implementation block
@property (nonatomic,retain) NSArray * gpuLocalEventList;                                                //@synthesize gpuLocalEventList=_gpuLocalEventList - In the implementation block
@property (nonatomic,retain) NSArray * gpuSharedEventList;                                               //@synthesize gpuSharedEventList=_gpuSharedEventList - In the implementation block
@property (nonatomic,readonly) unsigned long long* currentEventValue;                                    //@synthesize currentEventValue=_currentEventValue - In the implementation block
@property (assign,nonatomic) unsigned long long numDevicesToUse;                                         //@synthesize numDevicesToUse=_numDevicesToUse - In the implementation block
@property (assign,nonatomic) unsigned long long numDevicesUsedWithFirstBatch;                            //@synthesize numDevicesUsedWithFirstBatch=_numDevicesUsedWithFirstBatch - In the implementation block
@property (nonatomic,readonly) NSArray * deviceList;                                                     //@synthesize deviceList=_deviceList - In the implementation block
@property (nonatomic,readonly) int deviceType;                                                           //@synthesize deviceType=_deviceType - In the implementation block
@property (nonatomic,readonly) char isAppleSoCGPU;                                                       //@synthesize isAppleSoCGPU=_isAppleSoCGPU - In the implementation block
@property (nonatomic,readonly) NSMutableArray * gpuCommandQueueList;                                     //@synthesize gpuCommandQueueList=_gpuCommandQueueList - In the implementation block
@property (nonatomic,readonly) NSMutableArray * gpuCommandBufferList;                                    //@synthesize gpuCommandBufferList=_gpuCommandBufferList - In the implementation block
@property (nonatomic,readonly) NSMutableArray * gpuMPSCommandBufferList;                                 //@synthesize gpuMPSCommandBufferList=_gpuMPSCommandBufferList - In the implementation block
@property (nonatomic,readonly) NSMutableArray * gpuWriteTensorKernelList;                                //@synthesize gpuWriteTensorKernelList=_gpuWriteTensorKernelList - In the implementation block
@property (nonatomic,readonly) NSMutableArray * gpuReadTensorKernelList;                                 //@synthesize gpuReadTensorKernelList=_gpuReadTensorKernelList - In the implementation block
@property (nonatomic,readonly) NSMutableArray * gpuLibrary;                                              //@synthesize gpuLibrary=_gpuLibrary - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesForwardConcat2DArray;                     //@synthesize gpuPipelineStatesForwardConcat2DArray=_gpuPipelineStatesForwardConcat2DArray - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesForwardConcat2D;                          //@synthesize gpuPipelineStatesForwardConcat2D=_gpuPipelineStatesForwardConcat2D - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesConcatFloat;                              //@synthesize gpuPipelineStatesConcatFloat=_gpuPipelineStatesConcatFloat - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesConcatBool;                               //@synthesize gpuPipelineStatesConcatBool=_gpuPipelineStatesConcatBool - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesFill2DArray;                              //@synthesize gpuPipelineStatesFill2DArray=_gpuPipelineStatesFill2DArray - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesFill2D;                                   //@synthesize gpuPipelineStatesFill2D=_gpuPipelineStatesFill2D - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesPad2DArray;                               //@synthesize gpuPipelineStatesPad2DArray=_gpuPipelineStatesPad2DArray - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesPad2D;                                    //@synthesize gpuPipelineStatesPad2D=_gpuPipelineStatesPad2D - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesReduceAcrossBatch;                        //@synthesize gpuPipelineStatesReduceAcrossBatch=_gpuPipelineStatesReduceAcrossBatch - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesArithmeticUnaryForward;                   //@synthesize gpuPipelineStatesArithmeticUnaryForward=_gpuPipelineStatesArithmeticUnaryForward - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesArithmeticUnaryGradient;                  //@synthesize gpuPipelineStatesArithmeticUnaryGradient=_gpuPipelineStatesArithmeticUnaryGradient - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesArithmeticBinaryForward;                  //@synthesize gpuPipelineStatesArithmeticBinaryForward=_gpuPipelineStatesArithmeticBinaryForward - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesArithmeticBinaryGradient;                 //@synthesize gpuPipelineStatesArithmeticBinaryGradient=_gpuPipelineStatesArithmeticBinaryGradient - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesArithmeticReduceGradientAny;              //@synthesize gpuPipelineStatesArithmeticReduceGradientAny=_gpuPipelineStatesArithmeticReduceGradientAny - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesArithmeticReduceGradientAll;              //@synthesize gpuPipelineStatesArithmeticReduceGradientAll=_gpuPipelineStatesArithmeticReduceGradientAll - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesEmbeddingRenormalizeWeights;              //@synthesize gpuPipelineStatesEmbeddingRenormalizeWeights=_gpuPipelineStatesEmbeddingRenormalizeWeights - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesEmbeddingForward;                         //@synthesize gpuPipelineStatesEmbeddingForward=_gpuPipelineStatesEmbeddingForward - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesEmbeddingGradient;                        //@synthesize gpuPipelineStatesEmbeddingGradient=_gpuPipelineStatesEmbeddingGradient - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesCompareRelationalOpForward;               //@synthesize gpuPipelineStatesCompareRelationalOpForward=_gpuPipelineStatesCompareRelationalOpForward - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesCompareLogicalOpForward;                  //@synthesize gpuPipelineStatesCompareLogicalOpForward=_gpuPipelineStatesCompareLogicalOpForward - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesMHAMaskForward;                           //@synthesize gpuPipelineStatesMHAMaskForward=_gpuPipelineStatesMHAMaskForward - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesMemFillFloat;                             //@synthesize gpuPipelineStatesMemFillFloat=_gpuPipelineStatesMemFillFloat - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuPipelineStatesMemCopy;                                  //@synthesize gpuPipelineStatesMemCopy=_gpuPipelineStatesMemCopy - In the implementation block
@property (nonatomic,retain) NSMutableArray * gpuConcurrentEncoderFence;                                 //@synthesize gpuConcurrentEncoderFence=_gpuConcurrentEncoderFence - In the implementation block
+(id)filteredGPUListIncludingLowPoweredBuiltin:(char)arg1 ;
+(char)multiGPUNotSupportedInFilteredGPUList:(id)arg1 ;
+(unsigned long long)calculateImageSizeForTensor:(id)arg1 ;
-(id)copyWithZone:(NSZone*)arg1 ;
-(void)dealloc;
-(char)isEqual:(id)arg1 ;
-(NSString *)description;
-(int)deviceType;
-(NSMutableArray *)gpuPipelineStatesForwardConcat2D;
-(NSMutableArray *)gpuPipelineStatesForwardConcat2DArray;
-(NSArray *)deviceList;
-(NSMutableArray *)gpuLibrary;
-(id)fusedBatchNormalizationAndNeuronLayerWithDescriptor:(id)arg1 numOfFeatureChannels:(unsigned long long)arg2 mean:(id)arg3 variance:(id)arg4 beta:(id)arg5 gamma:(id)arg6 varianceEpsilon:(float)arg7 momentum:(float)arg8 ;
-(id)batchNormalizationLayerWithChannelCount:(unsigned long long)arg1 mean:(id)arg2 variance:(id)arg3 beta:(id)arg4 gamma:(id)arg5 varianceEpsilon:(float)arg6 momentum:(float)arg7 ;
-(char)compileLayerDeviceOps:(id)arg1 sourceTensors:(id)arg2 resultTensor:(id)arg3 ;
-(char)setNormalizationLayerOptimizerDataForDeviceOps:(id)arg1 beta:(id)arg2 gamma:(id)arg3 ;
-(id)scatterLayerWithDimension:(unsigned long long)arg1 reduceType:(int)arg2 ;
-(id)poolingLayerWithDescriptor:(id)arg1 paddingSizes:(unsigned long long*)arg2 ;
-(id)paddingLayerWithPaddingType:(int)arg1 paddingLeft:(unsigned long long)arg2 paddingRight:(unsigned long long)arg3 paddingTop:(unsigned long long)arg4 paddingBottom:(unsigned long long)arg5 constantValue:(float)arg6 ;
-(id)groupNormalizationLayerWithFeatureChannelCount:(unsigned long long)arg1 groupCount:(unsigned long long)arg2 beta:(id)arg3 gamma:(id)arg4 varianceEpsilon:(float)arg5 ;
-(id)selectLayer;
-(id)lossLayerWithDescriptor:(id)arg1 ;
-(id)fusedFullyConnectedAndNeuronLayerWithDescriptor:(id)arg1 convolutionDescriptor:(id)arg2 weights:(id)arg3 biasTerms:(id)arg4 ;
-(id)fullyConnectedLayerWithDescriptor:(id)arg1 weights:(id)arg2 biasTerms:(id)arg3 ;
-(char)setConvolutionLayerOptimizerDataForDeviceOps:(id)arg1 weights:(id)arg2 bias:(id)arg3 ;
-(id)createOptimizerDeviceDataForTensor:(id)arg1 optimizer:(id)arg2 optimizerData:(id)arg3 isVector:(char)arg4 ;
-(id)convolutionLayerWithDescriptor:(id)arg1 weights:(id)arg2 biasTerms:(id)arg3 ;
-(id)convolutionTransposeLayerWithDescriptor:(id)arg1 weights:(id)arg2 biasTerms:(id)arg3 ;
-(id)neuronLayerWithDescriptor:(id)arg1 ;
-(id)instanceNormalizationLayerWithChannelCount:(unsigned long long)arg1 mean:(id)arg2 variance:(id)arg3 beta:(id)arg4 gamma:(id)arg5 varianceEpsilon:(float)arg6 momentum:(float)arg7 ;
-(id)layerNormalizationLayerWithNormalizedShape:(id)arg1 beta:(id)arg2 gamma:(id)arg3 varianceEpsilon:(float)arg4 isFusedWithArithmeticLayer:(char)arg5 ;
-(id)arithmeticLayerWithOperation:(int)arg1 activationDescriptor:(id)arg2 ;
-(id)compareLayerWithOperation:(int)arg1 ;
-(id)reductionLayerWithReduceType:(int)arg1 dimensions:(id)arg2 sourceShapeCount:(unsigned long long)arg3 ;
-(id)softmaxLayerWithOperation:(int)arg1 dimension:(unsigned long long)arg2 sourceShapeCount:(unsigned long long)arg3 ;
-(id)lossYOLOLayerWithDescriptor:(id)arg1 ;
-(id)dropoutLayerWithRate:(float)arg1 seed:(unsigned long long)arg2 ;
-(id)reshapeLayerWithShape:(id)arg1 ;
-(id)transposeLayerWithShape:(id)arg1 ;
-(id)lstmLayerWithDescriptor:(id)arg1 inputWeights:(id)arg2 hiddenWeights:(id)arg3 peepholeWeights:(id)arg4 biasTerms:(id)arg5 gateActivations:(id)arg6 outputResultActivation:(id)arg7 ;
-(id)gramMatrixLayerWithScaleFactor:(float)arg1 ;
-(id)upsampleLayerWithScaleFactorX:(float)arg1 scaleFactorY:(float)arg2 sampleMode:(int)arg3 alignCorners:(char)arg4 ;
-(id)optimizerSGDWithDescriptor:(id)arg1 momentunScale:(float)arg2 useNesterovMomentum:(char)arg3 ;
-(id)optimizerAdamWithDescriptor:(id)arg1 beta1:(float)arg2 beta2:(float)arg3 epsilon:(float)arg4 timeStep:(unsigned long long)arg5 ;
-(id)optimizerRMSPropWithDescriptor:(id)arg1 momentumScale:(float)arg2 alpha:(float)arg3 epsilon:(float)arg4 centered:(char)arg5 ;
-(unsigned long long)allocatedDeviceDataSizeForTraining:(char)arg1 layer:(id)arg2 ;
-(void)fuseLayersForTrainingGraph:(id)arg1 stopGradientTensorList:(id)arg2 ;
-(void)fuseLayersForInferenceGraph:(id)arg1 startAtLayerIndex:(unsigned long long)arg2 ;
-(char)setLSTMLayerOptimizerDataForDeviceOps:(id)arg1 inputWeights:(id)arg2 hiddenWeights:(id)arg3 biasTerms:(id)arg4 ;
-(id)fusedConvolutionAndNeuronLayerWithDescriptor:(id)arg1 convolutionDescriptor:(id)arg2 weights:(id)arg3 biasTerms:(id)arg4 ;
-(id)fusedInstanceNormalizationAndNeuronLayerWithDescriptor:(id)arg1 numOfFeatureChannels:(unsigned long long)arg2 mean:(id)arg3 variance:(id)arg4 beta:(id)arg5 gamma:(id)arg6 varianceEpsilon:(float)arg7 momentum:(float)arg8 ;
-(id)multiheadAttentionLayerWithDescriptor:(id)arg1 weights:(id)arg2 bias:(id)arg3 attnBias:(id)arg4 inferenceOnly:(char)arg5 ;
-(char)setMHALayerOptimizerDataForDeviceOps:(id)arg1 optimizerDataForWeights:(id)arg2 optimizerDataForBias:(id)arg3 ;
-(id)matMulLayerWithDescriptor:(id)arg1 sourceTensors:(id)arg2 resultTensor:(id)arg3 inferenceOnly:(char)arg4 ;
-(id)embeddingLayerWithDescriptor:(id)arg1 weights:(id)arg2 inferenceOnly:(char)arg3 ;
-(id)sliceLayerWithbegin:(id)arg1 end:(id)arg2 stride:(id)arg3 inferenceOnly:(char)arg4 ;
-(void)setConvolutionGradientComputeWeightsAndBiasOnly:(id)arg1 ;
-(void)setFullyConnectedGradientComputeWeightsAndBiasOnly:(id)arg1 ;
-(id)weightsGradients:(id)arg1 ;
-(id)biasesGradients:(id)arg1 ;
-(id)betaGradients:(id)arg1 ;
-(id)gammaGradients:(id)arg1 ;
-(id)mhaWeightGradient:(id)arg1 withSize:(unsigned long long)arg2 index:(unsigned long long)arg3 ;
-(id)mhaBiasGradient:(id)arg1 withSize:(unsigned long long)arg2 index:(unsigned long long)arg3 ;
-(id)mhaAttnBiasGradient:(id)arg1 withSize:(unsigned long long)arg2 index:(unsigned long long)arg3 ;
-(id)embeddingWeightsGradients:(id)arg1 embeddingCount:(unsigned long long)arg2 embeddingDimension:(unsigned long long)arg3 ;
-(id)lstmInputWeightGradient:(id)arg1 mlcWeightIndex:(unsigned long long)arg2 ;
-(id)lstmHiddenWeightGradient:(id)arg1 mlcWeightIndex:(unsigned long long)arg2 ;
-(id)lstmBiasGradient:(id)arg1 mlcBiasIndex:(unsigned long long)arg2 ;
-(void)allocateParameterGradientsForDeviceOps:(id)arg1 parameters:(id)arg2 ;
-(id)splitLayerWithDimension:(unsigned long long)arg1 ;
-(id)gatherLayerWithDimension:(unsigned long long)arg1 ;
-(char)compileLayerDeviceOps:(id)arg1 sourceTensors:(id)arg2 resultTensors:(id)arg3 ;
-(unsigned long long)deviceMemorySizeForTensor:(id)arg1 ;
-(unsigned long long)numDevices;
-(void)updateTensorsForThreeFusedLayers:(id)arg1 layerNext:(id)arg2 layerNext2:(id)arg3 ;
-(char)isResultTensorInStopGradientTensorList:(id)arg1 resultTensor:(id)arg2 forInference:(char)arg3 ;
-(void)updateTensorsForFusedPaddingAndConvolutionLayer:(id)arg1 layerNext:(id)arg2 ;
-(char)canFuseConvolutionLayerForInference:(id)arg1 ;
-(void)updateTensorsForTwoFusedLayers:(id)arg1 layerNext:(id)arg2 ;
-(char)doesActivationRequireInput:(id)arg1 forInference:(char)arg2 ;
-(void)updateTensorsForFMAFusedLayers:(id)arg1 layerNext:(id)arg2 ;
-(void)updateTensorsForFusedArithmeticAndLayerNormalizationLayer:(id)arg1 layerNext:(id)arg2 ;
-(void)fuseLayersForGraph:(id)arg1 stopGradientTensorList:(id)arg2 startAtLayerIndex:(unsigned long long)arg3 forInference:(char)arg4 ;
-(unsigned long long)numDevicesToUse;
-(NSMutableArray *)gpuCommandQueueList;
-(char)canFuseFullyConnectedLayerForInference:(id)arg1 ;
-(id)initWithDeviceList:(id)arg1 ;
-(id)initWithType:(int)arg1 gpuDeviceList:(id)arg2 gpuLocalEventList:(id)arg3 gpuSharedEventList:(id)arg4 currentEventValue:(unsigned long long*)arg5 gpuCommandQueueList:(id)arg6 gpuCommandBufferList:(id)arg7 gpuMPSCommandBufferList:(id)arg8 ;
-(char)createPipelineStatesForMissingActivationFunctions;
-(void)setGpuPipelineStatesForwardConcat2DArray:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesForwardConcat2D:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesConcatFloat:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesConcatBool:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesFill2DArray:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesFill2D:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesPad2DArray:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesPad2D:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesReduceAcrossBatch:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesArithmeticUnaryForward:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesArithmeticUnaryGradient:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesArithmeticBinaryForward:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesArithmeticBinaryGradient:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesArithmeticReduceGradientAny:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesArithmeticReduceGradientAll:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesEmbeddingRenormalizeWeights:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesEmbeddingForward:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesEmbeddingGradient:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesCompareRelationalOpForward:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesCompareLogicalOpForward:(NSMutableArray *)arg1 ;
-(void)setGpuConcurrentEncoderFence:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesMHAMaskForward:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesMemFillFloat:(NSMutableArray *)arg1 ;
-(void)setGpuPipelineStatesMemCopy:(NSMutableArray *)arg1 ;
-(NSMutableArray *)gpuPipelineStatesConcatFloat;
-(NSMutableArray *)gpuPipelineStatesConcatBool;
-(NSMutableArray *)gpuPipelineStatesFill2DArray;
-(NSMutableArray *)gpuPipelineStatesFill2D;
-(NSMutableArray *)gpuPipelineStatesPad2DArray;
-(NSMutableArray *)gpuPipelineStatesPad2D;
-(NSMutableArray *)gpuPipelineStatesReduceAcrossBatch;
-(NSMutableArray *)gpuPipelineStatesArithmeticUnaryForward;
-(NSMutableArray *)gpuPipelineStatesArithmeticUnaryGradient;
-(NSMutableArray *)gpuPipelineStatesArithmeticBinaryForward;
-(NSMutableArray *)gpuPipelineStatesArithmeticBinaryGradient;
-(NSMutableArray *)gpuPipelineStatesArithmeticReduceGradientAny;
-(NSMutableArray *)gpuPipelineStatesArithmeticReduceGradientAll;
-(NSMutableArray *)gpuPipelineStatesEmbeddingRenormalizeWeights;
-(NSMutableArray *)gpuPipelineStatesEmbeddingForward;
-(NSMutableArray *)gpuPipelineStatesEmbeddingGradient;
-(NSMutableArray *)gpuPipelineStatesCompareRelationalOpForward;
-(NSMutableArray *)gpuPipelineStatesCompareLogicalOpForward;
-(NSMutableArray *)gpuPipelineStatesMHAMaskForward;
-(NSMutableArray *)gpuPipelineStatesMemFillFloat;
-(NSMutableArray *)gpuPipelineStatesMemCopy;
-(NSMutableArray *)gpuConcurrentEncoderFence;
-(NSMutableArray *)gpuCommandBufferList;
-(unsigned long long*)currentEventValue;
-(NSArray *)gpuLocalEventList;
-(NSArray *)gpuSharedEventList;
-(NSMutableArray *)gpuMPSCommandBufferList;
-(void)allocateCommandBufferForDeviceAtIndex:(unsigned long long)arg1 ;
-(void)updateGraphExecutionTime:(id)arg1 atIndex:(int)arg2 gpuTime:(double)arg3 ;
-(void)signalNextEvent;
-(void)waitForOthers;
-(void)readTensor:(id)arg1 targetBuffer:(void*)arg2 ;
-(void)commitDeviceAtIndex:(unsigned long long)arg1 ;
-(double*)executionTimeInterval;
-(void)commitAndWaitForCompletion:(/*^block*/id)arg1 enableProfiling:(char)arg2 graphExecutionTime:(id)arg3 graphResultTensor:(id)arg4 ;
-(NSMutableArray *)gpuHeapTemporaryResourceAllocator;
-(char)needToAllocateDeviceMemoryForTensor:(id)arg1 batchSize:(unsigned long long)arg2 ;
-(void)allocateDeviceMemoryForTensor:(id)arg1 batchSize:(unsigned long long)arg2 ;
-(id)allocateDeviceMemoryForTensor:(id)arg1 device:(id)arg2 count:(unsigned long long)arg3 ;
-(void)selectDevicesWithBatchSize:(unsigned long long)arg1 calledfromBindAndWrite:(char)arg2 ;
-(unsigned long long)numDevicesUsedWithFirstBatch;
-(void)setNumDevicesUsedWithFirstBatch:(unsigned long long)arg1 ;
-(void)setNumDevicesToUse:(unsigned long long)arg1 ;
-(id)readTensor:(id)arg1 fromDeviceIndex:(unsigned long long)arg2 batchSize:(unsigned long long)arg3 ;
-(void)readTensor:(id)arg1 fromDeviceIndex:(unsigned long long)arg2 targetBuffer:(void*)arg3 batchSize:(unsigned long long)arg4 ;
-(void)multiDeviceTensorReduction:(id)arg1 sourceBuffer:(void*)arg2 targetBuffer:(void*)arg3 ;
-(void)readTensor:(id)arg1 fromDeviceIndex:(unsigned long long)arg2 targetBuffer:(void*)arg3 batchSize:(unsigned long long)arg4 reduceOverBatch:(char)arg5 ;
-(void)dispatchReadTensorFromAllDevices:(id)arg1 targetBuffer:(void*)arg2 batchSize:(unsigned long long)arg3 ;
-(void)broadcastTensor:(id)arg1 batchSize:(unsigned long long)arg2 ;
-(void)dispatchBroadcastTensor:(id)arg1 batchSize:(unsigned long long)arg2 ;
-(char)isAppleSoCGPU;
-(void)unsafe_signalAllDevicesExcludingDevice:(unsigned long long)arg1 eventValue:(unsigned long long)arg2 ;
-(void)waitForAllDevicesExcludingDevice:(unsigned long long)arg1 eventValue:(unsigned long long)arg2 ;
-(char)needToAllocateDeviceMemoryForTensor:(id)arg1 ;
-(void)allocateDeviceMemoryForTensor:(id)arg1 ;
-(void)deallocateDeviceMemoryForTensor:(id)arg1 ;
-(char)shareDeviceMemoryWithResultTensor:(id)arg1 sourceTensor:(id)arg2 ;
-(id)readTensor:(id)arg1 ;
-(id)readTensor:(id)arg1 fromDeviceIndex:(unsigned long long)arg2 ;
-(void)dispatchReadTensor:(id)arg1 targetBuffer:(void*)arg2 batchSize:(unsigned long long)arg3 ;
-(void)broadcastTensor:(id)arg1 ;
-(void)dispatchBroadcastTensor:(id)arg1 ;
-(char)synchronizeTensor:(id)arg1 ;
-(void)commitWithProfiling:(char)arg1 graphExecutionTime:(id)arg2 ;
-(void)commitAndWaitForCompletion:(/*^block*/id)arg1 ;
-(void)commitWithCompletionHandler:(/*^block*/id)arg1 enableProfiling:(char)arg2 graphExecutionTime:(id)arg3 graphResultTensor:(id)arg4 ;
-(void)signalAllDevicesExcludingDevice:(unsigned long long)arg1 eventValue:(unsigned long long)arg2 ;
-(char)transferTensor:(id)arg1 fromDevice:(id)arg2 ;
-(void)setDeviceMemoryForTensor:(id)arg1 data:(id)arg2 ;
-(void)allocateDeviceHeapForGraph:(id)arg1 forInference:(char)arg2 ;
-(void)selectDevicesWithBatchSize:(unsigned long long)arg1 ;
-(char)synchronizeTensorOnHost:(id)arg1 ;
-(id)initWithType:(int)arg1 selectsMultipleComputeDevices:(char)arg2 ;
-(void)allReduceOverXGMI:(id)arg1 deviceIndex:(unsigned long long)arg2 stateBuffers:(id)arg3 ;
-(void)setGpuHeapTemporaryResourceAllocator:(NSMutableArray *)arg1 ;
-(NSMutableArray *)gpuWriteTensorKernelList;
-(NSMutableArray *)gpuReadTensorKernelList;
-(os_unfair_lock_s)l;
-(void)setL:(os_unfair_lock_s)arg1 ;
-(void)setGpuLocalEventList:(NSArray *)arg1 ;
-(void)setGpuSharedEventList:(NSArray *)arg1 ;
-(void)encodePrimitiveToCommandBuffer:(id)arg1 gpuDeviceOps:(id)arg2 sourceTensor:(id)arg3 secondaryTensor:(id)arg4 tertiaryTensor:(id)arg5 resultTensor:(id)arg6 params:(CompareParams*)arg7 sizeOfParams:(unsigned long long)arg8 pipelineState:(id)arg9 deviceIndex:(unsigned long long)arg10 ;
-(void)dispatchFillTensor:(id)arg1 deviceIndex:(unsigned long long)arg2 ;
-(void)dispatchForwardSplitLayer:(id)arg1 sourceTensor:(id)arg2 resultTensors:(id)arg3 forConcat:(char)arg4 ;
-(void)dispatchGradientSplitLayer:(id)arg1 sourceGradientTensors:(id)arg2 resultGradientTensor:(id)arg3 forConcat:(char)arg4 ;
-(void)dispatchForwardArithmeticBinaryKernel:(id)arg1 sourceTensor:(id)arg2 sourceSecondaryTensor:(id)arg3 resultTensor:(id)arg4 deviceIndex:(unsigned long long)arg5 ;
-(void)reduceAcrossBatchForSource:(id)arg1 result:(id)arg2 batchSize:(unsigned long long)arg3 batchStride:(unsigned long long)arg4 numEntries:(unsigned long long)arg5 deviceIndex:(unsigned long long)arg6 commandBuffer:(id)arg7 ;
-(void)dispatchForwardActivationsKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchForwardPaddingKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchForwardSoftmaxKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchForwardLayerNormalizationKernel:(id)arg1 sourceTensors:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 forTraining:(char)arg5 ;
-(void)dispatchForwardBatchNormalizationKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 forTraining:(char)arg5 ;
-(void)dispatchForwardInstanceNormalizationKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 forTraining:(char)arg5 ;
-(void)dispatchForwardGroupNormalizationKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 forTraining:(char)arg5 ;
-(void)dispatchForwardResizeKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchTransposeKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 forward:(char)arg5 ;
-(void)dispatchForwardPoolingKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 forTraining:(char)arg5 ;
-(void)dispatchForwardGramMatrixKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 forTraining:(char)arg5 ;
-(void)dispatchForwardArithmeticTertiaryKernel:(id)arg1 sourceTensor:(id)arg2 sourceSecondaryTensor:(id)arg3 sourceTertiaryTensor:(id)arg4 resultTensor:(id)arg5 deviceIndex:(unsigned long long)arg6 ;
-(void)dispatchForwardArithmeticUnaryKernel:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientArithmeticBinaryKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 secondaryResultGradientTensor:(id)arg4 deviceIndex:(unsigned long long)arg5 ;
-(void)dispatchGradientSoftmaxKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientActivationsKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientPaddingKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientLayerNormalizationKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientBatchNormalizationKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientInstanceNormalizationKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientGroupNormalizationKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientResizeKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientPoolingKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientGramMatrixKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchGradientArithmeticUnaryKernel:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(void)dispatchForwardReshapeLayer:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultStateIsTemporary:(char)arg5 resultTensorAllocate:(char)arg6 forTraining:(char)arg7 ;
-(void)dispatchPadChannelsKernel:(id)arg1 sourceImageBatch:(id)arg2 resultImageBatch:(id)arg3 deviceIndex:(unsigned long long)arg4 ;
-(char)isResultTensorUsedForGradientComputationByLayer:(id)arg1 ;
-(char)isDeviceMemorySharableBetweenSourceAndGradientTensor:(id)arg1 sourceTensor:(id)arg2 ;
-(void)dispatchFillKernel:(id)arg1 imageBatch:(id)arg2 deviceIndex:(unsigned long long)arg3 ;
-(void)dispatchForwardCompareLayer:(id)arg1 sourceTensor:(id)arg2 secondaryTensor:(id)arg3 resultTensor:(id)arg4 compareOp:(int)arg5 resultTensorIsTemporary:(char)arg6 resultTensorAllocate:(char)arg7 forTraining:(char)arg8 ;
-(void)dispatchForwardSplitLayer:(id)arg1 sourceTensor:(id)arg2 resultTensors:(id)arg3 ;
-(void)dispatchGradientSplitLayer:(id)arg1 sourceGradientTensors:(id)arg2 resultGradientTensor:(id)arg3 ;
-(void)dispatchForwardConcatLayer:(id)arg1 sourceTensors:(id)arg2 resultTensor:(id)arg3 ;
-(void)dispatchGradientConcatLayer:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensors:(id)arg3 ;
-(void)dispatchForwardReduceLayer:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 reductionType:(int)arg4 reduceDimensions:(id)arg5 resultTensorIsTemporary:(char)arg6 resultTensorAllocate:(char)arg7 forTraining:(char)arg8 ;
-(void)dispatchGradientReduceLayer:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 reductionType:(int)arg4 reduceDimensions:(id)arg5 resultTensorIsTemporary:(char)arg6 resultTensorAllocate:(char)arg7 ;
-(void)dispatchForwardSelectLayer:(id)arg1 conditionTensor:(id)arg2 sourceTensors:(id)arg3 resultTensor:(id)arg4 resultTensorIsTemporary:(char)arg5 resultTensorAllocate:(char)arg6 forTraining:(char)arg7 ;
-(void)dispatchGradientSelectLayer:(id)arg1 conditionTensor:(id)arg2 sourceGradientTensor:(id)arg3 resultGradientTensors:(id)arg4 resultTensorIsTemporary:(char)arg5 resultTensorAllocate:(char)arg6 ;
-(void)dispatchForwardScatterLayer:(id)arg1 sourceTensors:(id)arg2 resultTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 forTraining:(char)arg6 ;
-(void)dispatchForwardGatherLayer:(id)arg1 sourceTensors:(id)arg2 resultTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 forTraining:(char)arg6 ;
-(void)dispatchForwardEmbeddingLayer:(id)arg1 weight:(id)arg2 sourceTensor:(id)arg3 resultTensor:(id)arg4 ;
-(void)dispatchGradientEmbeddingLayer:(id)arg1 sourceGradientTensor:(id)arg2 ;
-(void)dispatchForwardSliceLayer:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 resultTensorAllocate:(char)arg4 forTraining:(char)arg5 ;
-(void)dispatchForwardFullyConnectedLayer:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 forTraining:(char)arg6 ;
-(void)dispatchGradientFullyConnectedLayer:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultStateIsTemporary:(char)arg5 resultTensorAllocate:(char)arg6 ;
-(void)dispatchForwardLayer:(id)arg1 sourceTensor:(id)arg2 resultTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultStateIsTemporary:(char)arg5 resultTensorAllocate:(char)arg6 forTraining:(char)arg7 ;
-(void)dispatchForwardLayer:(id)arg1 sourceTensor:(id)arg2 secondaryTensor:(id)arg3 tertiaryTensor:(id)arg4 resultTensor:(id)arg5 resultTensorIsTemporary:(char)arg6 resultStateIsTemporary:(char)arg7 resultTensorAllocate:(char)arg8 forTraining:(char)arg9 ;
-(void)dispatchForwardFusedArithmeticLayerNormalizationLayer:(id)arg1 sourceTensors:(id)arg2 resultTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 forTraining:(char)arg6 ;
-(void)dispatchForwardMatMulLayer:(id)arg1 sourceTensors:(id)arg2 resultTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 ;
-(void)dispatchGradientMatMulLayer:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensors:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 ;
-(void)dispatchForwardMHALayer:(id)arg1 sourceTensors:(id)arg2 resultTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultStateIsTemporary:(char)arg5 resultTensorAllocate:(char)arg6 forTraining:(char)arg7 ;
-(void)dispatchGradientMHALayer:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensors:(id)arg3 resultTensorIsTemporary:(char)arg4 resultStateIsTemporary:(char)arg5 resultTensorAllocate:(char)arg6 ;
-(void)dispatchForwardLossLayer:(id)arg1 sourceTensor:(id)arg2 labelsTensor:(id)arg3 labelsTensorStride:(unsigned long long)arg4 weightsTensor:(id)arg5 resultTensor:(id)arg6 resultTensorIsTemporary:(char)arg7 resultStateIsTemporary:(char)arg8 resultTensorAllocate:(char)arg9 forTraining:(char)arg10 ;
-(void)dispatchGradientLayer:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 ;
-(void)dispatchGradientLayer:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 secondaryResultGradientTensor:(id)arg4 resultTensorIsTemporary:(char)arg5 resultTensorAllocate:(char)arg6 ;
-(void)dispatchGradientReshapeLayer:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 ;
-(void)dispatchGradientSliceLayer:(id)arg1 sourceGradientTensor:(id)arg2 resultGradientTensor:(id)arg3 resultTensorAllocate:(char)arg4 ;
-(void)dispatchGradientLossLayer:(id)arg1 sourceGradientTensor:(id)arg2 labelsTensor:(id)arg3 labelsTensorStride:(unsigned long long)arg4 weightsTensor:(id)arg5 resultGradientTensor:(id)arg6 resultTensorIsTemporary:(char)arg7 resultTensorAllocate:(char)arg8 ;
-(void)dispatchForwardAndGradientLossLayer:(id)arg1 sourceTensor:(id)arg2 labelsTensor:(id)arg3 labelsTensorStride:(unsigned long long)arg4 weightsTensor:(id)arg5 resultTensor:(id)arg6 resultGradientTensor:(id)arg7 resultTensorIsTemporary:(char)arg8 resultTensorAllocate:(char)arg9 ;
-(char)shareSourceAndResultGradientTensorDeviceMemory:(id)arg1 sourceTensor:(id)arg2 resultGradientTensor:(id)arg3 ;
-(void)optimizeComputationForTrainingGraph:(id)arg1 ;
-(void)dispatchRNNForwardLayer:(id)arg1 sourceTensors:(id)arg2 resultTensors:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 ;
-(void)dispatchRNNForwardLayer:(id)arg1 sourceTensors:(id)arg2 resultTensors:(id)arg3 resultTensorIsTemporary:(char)arg4 resultStateIsTemporary:(char)arg5 resultTensorAllocate:(char)arg6 forTraining:(char)arg7 ;
-(void)dispatchRNNGradientLayer:(id)arg1 sourceGradientTensors:(id)arg2 resultGradientTensors:(id)arg3 resultTensorIsTemporary:(char)arg4 resultTensorAllocate:(char)arg5 ;
-(void)incrementReadCountForTensorDeviceMemory:(id)arg1 increment:(long long)arg2 ;
-(void)incrementReadCountForGradientState:(id)arg1 increment:(long long)arg2 ;
-(void)sumSharedGradientsForNormalizationLayerTensorParameter:(id)arg1 layerIndexForSummedGradients:(unsigned long long)arg2 isBetaTensor:(char)arg3 ;
-(void)setOptimizerLearningRate:(id)arg1 learningRate:(float)arg2 ;
-(void)setOptimizerTimeStep:(id)arg1 timeStep:(unsigned long long)arg2 ;
-(void)sumSharedGradientsForConvolutionLayerTensorParameter:(id)arg1 layerIndexForSummedGradients:(unsigned long long)arg2 ;
-(void)updateConvolutionLayer:(id)arg1 optimizer:(id)arg2 weightsParameter:(id)arg3 biasesParameter:(id)arg4 ;
-(void)updateFullyConnectedLayer:(id)arg1 optimizer:(id)arg2 weightsParameter:(id)arg3 biasesParameter:(id)arg4 ;
-(void)updateBatchNormalizationLayer:(id)arg1 optimizer:(id)arg2 betaParameter:(id)arg3 gammaParameter:(id)arg4 meanTensor:(id)arg5 varianceTensor:(id)arg6 ;
-(void)updateInstanceNormalizationLayer:(id)arg1 optimizer:(id)arg2 betaParameter:(id)arg3 gammaParameter:(id)arg4 ;
-(void)updateLayerNormalizationLayer:(id)arg1 optimizer:(id)arg2 betaParameter:(id)arg3 gammaParameter:(id)arg4 ;
-(void)updateGroupNormalizationLayer:(id)arg1 optimizer:(id)arg2 betaParameter:(id)arg3 gammaParameter:(id)arg4 ;
-(void)updateRNNLayer:(id)arg1 optimizer:(id)arg2 inputWeightsParameter:(id)arg3 hiddenWeightsParameter:(id)arg4 biasesParameter:(id)arg5 ;
-(void)updateMultiheadAttentionLayer:(id)arg1 optimizer:(id)arg2 weightsParameter:(id)arg3 biasesParameter:(id)arg4 ;
-(void)updateEmbeddingLayer:(id)arg1 weightsParameter:(id)arg2 optimizer:(id)arg3 ;
-(void)updateTensorParameter:(id)arg1 optimizer:(id)arg2 gradient:(id)arg3 ;
-(void)synchronizeUpdatesForLayer:(id)arg1 ;
-(void)synchronizeOptimizerUpdatesForTensor:(id)arg1 ;
-(void)convertUpdatesToTensorDataForLayer:(id)arg1 ;
-(void)convertUpdatesToTensorDataForTensorParameters:(id)arg1 ;
-(void)reloadParameterDataFromHostToDeviceMemoryForTensor:(id)arg1 ;
-(id)getGradientBufferForNormalizationState:(id)arg1 layer:(id)arg2 isBetaTensor:(char)arg3 ;
-(void)setOptimizerTimeStepToMPSKernel:(id)arg1 ;
-(void)updateWithOptimizer:(id)arg1 commandBuffer:(id)arg2 deviceParameter:(id)arg3 source:(id)arg4 gradient:(id)arg5 result:(id)arg6 momentumIndex:(unsigned long long)arg7 ;
-(void)saveOrRestoreTempMatrixDisableUpdates:(id)arg1 commandBuffer:(id)arg2 auxiliaryWeightsMemory:(id)arg3 auxiliaryMomentumMemory:(id)arg4 auxiliaryVelocityMemory:(id)arg5 auxiliaryCenterWeightMemory:(id)arg6 deviceNumber:(unsigned long long)arg7 kernelNumber:(unsigned long long)arg8 mlcIndex:(unsigned long long)arg9 auxIndex:(unsigned long long)arg10 numOptimizerData:(unsigned long long)arg11 saveToAux:(char)arg12 isInputWeight:(char)arg13 isHiddenWeight:(char)arg14 isBias:(char)arg15 ;
-(void)synchronizeOptimizerBuffers:(id)arg1 commandBuffer:(id)arg2 ;
-(void)synchronizeWeightMatrixForRNNLayer:(id)arg1 matrixId:(unsigned long long)arg2 parameterType:(unsigned long long)arg3 accumulatorIndex:(unsigned long long)arg4 forLSTMNum:(unsigned long long)arg5 forDeviceNum:(unsigned long long)arg6 forGate:(unsigned long long)arg7 ;
-(char)checkToConvertTensor:(id)arg1 inLayer:(id)arg2 ;
-(void)rotateAndCopyMTLBuffer:(id)arg1 toNSData:(id)arg2 withTensorDescriptor:(id)arg3 ;
-(void)copyMTLBuffer:(id)arg1 toNSData:(id)arg2 ;
-(void)copyMTLBuffer:(id)arg1 toBytes:(void*)arg2 length:(unsigned long long)arg3 ;
-(void)reloadLSTMParameters:(id)arg1 rnnGPUDeviceOps:(id)arg2 mlcParameterIndex:(unsigned long long)arg3 tensor:(id)arg4 isInputWeight:(char)arg5 isHiddenWeight:(char)arg6 isBias:(char)arg7 deviceNumber:(unsigned long long)arg8 ;
@end

